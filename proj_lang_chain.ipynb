{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_initalization(model, max_new_tokens, top_k, top_p, temperature):\n",
    "    # global processed\n",
    "    urls = {'gpt2':\"http://127.0.0.1:5000/own_gpt2\"}\n",
    "    url = urls.get(model.lower(), urls['gpt2'])\n",
    "    llm = HuggingFaceTextGenInference(inference_server_url = url,\n",
    "                                      max_new_tokens = max_new_tokens,\n",
    "                                      top_k = top_k,\n",
    "                                      top_p = top_p,\n",
    "                                      temperature = temperature,\n",
    "                                      callbacks = [StreamingStdOutCallbackHandler()]\n",
    "                                      )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extract_pypdf(file):\n",
    "    '''This is a simple method to extract text from PDF using pypdf'''\n",
    "    try:\n",
    "        text = str()\n",
    "        pdf = pypdf.PdfReader(file)\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "    except Exception as ep:\n",
    "        return f'Something went wrong {ep}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunk(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path ='/home/laxmidhar/Coding Stuff/AI Models/GTE-Base/'\n",
    "model_path = '/home/laxmidhar/Coding Stuff/AI Models/e5-large-v2/'\n",
    "embedings = HuggingFaceEmbeddings(model_name = model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(text_chunks):\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding = embedings)\n",
    "    vector_store.save_local('/home/laxmidhar/Coding Stuff/Vector Database/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_chain():\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the providede context, make sure to provide all the details if the answer is not in provided\n",
    "    context just say, \"Answer is not available in the context\", dont't provide the wrong answer\\n\\n\n",
    "    Context:\\n{context}?\\n\n",
    "    Question:\\n{question}\\n\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    model = llm_initalization('gpt2', max_new_tokens=512, top_k = 50, top_p = 0.95, temperature = 0.4)\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])\n",
    "    chain = load_qa_chain(model, chain_type='stuff', prompt = prompt)\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QnA(user_question):\n",
    "    new_DB = FAISS.load_local('/home/laxmidhar/Coding Stuff/Vector Database/', embedings, allow_dangerous_deserialization = True)\n",
    "    docs = new_DB.similarity_search(user_question)\n",
    "    chains = get_conversational_chain()\n",
    "    response = chains({'input_documents':docs, 'question':user_question})#, retun_only_outputs = True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file, query):\n",
    "    raw_text = text_extract_pypdf(file)\n",
    "    text_chunk = get_text_chunk(raw_text)\n",
    "    get_vector_store(text_chunk)\n",
    "    answer = QnA(query)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/home/laxmidhar/Coding/data/6.3.+Modifying+strings.pdf'\n",
    "question = 'Which method change all first letters to uppercase'\n",
    "main(file, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Extra\n",
    "import requests\n",
    "from typing import Any, List, Mapping, Optional\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.llms.base import LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaLLM(LLM):\n",
    "\n",
    "    class Config:\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self, model) -> str:\n",
    "        return model\n",
    "\n",
    "    def _call(self, query: str, context = None, run_manager: Optional[CallbackManagerForLLMRun] = None, max_length = 512, temperature = 0.3, **kwargs: Any) -> str:\n",
    "        api_url = \"http://127.0.0.1:5000/own_gpt2\"\n",
    "        data = {\"context\": context, \"question\": query, \"max_length\": max_length, \"temperature\": temperature}\n",
    "        response = requests.post(url = api_url, json = data)\n",
    "        result = response.json()\n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"llmUrl\": self.llm_url}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LlamaLLM()\n",
    "#Testing\n",
    "prompt = \"Question: Who is Albert Einstein? \\n Answer:\"\n",
    "result = llm._call(prompt)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
